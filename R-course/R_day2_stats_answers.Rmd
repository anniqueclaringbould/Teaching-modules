---
title: "R course day 2: Statistics"
author: "Niek de Klein, Annique Claringbould"
output: 
  html_document:
    toc: true # table of content true
    depth: 3  # upto three depths of headings (specified by #, ## and ###)
    number_sections: true  ## if you want number sections at each table header
    highlight: tango  # specifies the syntax highlighting style
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


<br><br><br>

# Introduction

Today, we will continue the R course with some basic statistics. We will use two datasets to go through a number of exercises and questions today.<br>
First we will use a dataset of flower petal measurements called [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set), this dataset is inherent to R and meant for practice. You do not need to load or import the data, you can get started immediately. You can find many examples of people using this dataset online, so it is useful if you have an idea of what it looks like.<br>
The next dataset is gene expression data from an actual experiment on a B cell line derived from publicly available sample called 'GM12878'.<br>

We will explore these datasets and apply some of the statistics you have learned about previously. 
<br><br>

## Exploring the data
The following commands are useful for exploring data:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
head() #prints first 10 rows of a dataset
dim() #gives the dimensions (rows and columns) of a dataset
colnames() #prints the column names of a dataset
rownames() #prints the row names of a dataset
View() #opens the dataset in the top window of RStudio
str() #prints an overview of the 'structure' of the data, e.g. the column names, their data types
summary() #gives a summary of each column, specific output depends on data types in each column
table() #gives a count table about a particular variable
```
<br>

**Q1**: As **iris** is a default dataset that is included with R, you can start exploring the data without loading it first:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
head(iris)
```

* What measurements does **iris** contain?
* What information does each of the columns describe? *Hint* Find more information about [sepal](https://en.wikipedia.org/wiki/Sepal) and [petal](https://en.wikipedia.org/wiki/Petal)
* What is the unit of measurement? *Hint* Here is some basic information about the dataset: [iris](https://en.wikipedia.org/wiki/Iris_flower_data_set)
* How many flower measurements are recoreded?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 1
#Code
head(iris)
dim(iris)

#Answer
#Length and width of 'Sepal' (green leaves just underneath flower), of 'Petal' (flower leaves) and the species of Iris in cm
#There are 150 observations
```
<br>

**Q2**: What species of iris are tested? And how many observations are there in each species? Access the relevant column using `iris$Species` and use one of the commands mentioned in the overview above.

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 2
#Code
table(iris$Species)

#Answer
#There are 50 observations for each of setosa, versicolor and virginica
```
<br><br>

# Differences between 2 groups
We would like to know if the petal length of *Iris virginica* is longer than the petal length of *Iris versicolor*. When asking these questions usually it refers to the averages of the groups. So first, look at the mean of both groups:

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
mean(iris[iris$Species == 'virginica',]$Petal.Length)
```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
mean(iris[iris$Species == 'versicolor',]$Petal.Length)
```
<br>

**Q3**: What does this part of the code above do: `iris[iris$Species == 'virginica',]`?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 3
#Code
iris[iris$Species == 'virginica',]

#Answer
#It selects only the flowers of species 'virginica' from the dataframe
```
<br>

Save the differences between the two means in variable `diff`:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
# Save the difference in the *variable* diff
diff <- mean(iris[iris$Species == 'virginica',]$Petal.Length) - mean(iris[iris$Species == 'versicolor',]$Petal.Length)
```
<br>

**Q4**: What is the value of `diff`? Why would we need P-values and confidence intervals if we can already see that there is a difference in petal length?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 4
#Code
diff <- mean(iris[iris$Species == 'virginica',]$Petal.Length) - mean(iris[iris$Species == 'versicolor',]$Petal.Length)
diff

#Answer
#You want to know if this difference could have occurred by chance (that is, if you had picked 50 different versicolor flowers, would you have gotten the same result?)
```
<br><br>

## Parametric: two-sample t-test

We have already used `iris[iris$Species == 'virginica',]$Petal.length` a few times, but these long statements are not very readable. Save the petal lengths for *virginica* in a variable called `virg_pl`:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
virg_pl <- iris[iris$Species == 'virginica',]$Petal.Length
```
<br>
Do the same for *versicolor* (`vers_pl`) and *setosa* (`set_pl`).
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=FALSE}
virg_pl <- iris[iris$Species == 'virginica',]$Petal.Length
vers_pl <- iris[iris$Species == 'versicolor',]$Petal.Length
set_pl <- iris[iris$Species == 'setosa',]$Petal.Length
```
<br>

You may have covered how to manually calculate the P-value from a t-test using the mean and standard deviation in your program, but here we show how to simply do it in R using the function `t.test()`.<br>

A t-test is a parametric test, and therefore is assumes that the length of the petals is approximatly normally distributed for the population. To assess this assumption, it would be good to plot the data and have a look at it. We will go into that tomorrow.<br>

**Q5**: Run a t-test to compare the petal length between virginica and versicolor:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
t.test(virg_pl, vers_pl)
```
What is our null hypothesis, and given our t-test result, can we reject it?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 5
#Code
t.test(virg_pl, vers_pl)

#Answer
#The assumption is that both groups are sampled from normal distributions with the same variance. 
#The null hypothesis is that the two means are equal (no difference in length in petal length between the two species).
#Because p-value < 0.05 we reject the null hypothesis.
```
<br>

You can save the result of the t-test by assigning it to a variable using `variable <- t.test()`, like we did a number of times in yesterdays practical.
<br> 

**Q6**:

* Run a t-test to compare the petal length between *virginica* and *setosa*
* Assign the result to `t_test_virg_set`
* Use `t_test_virg_set$p.value` to find the exact p-value of this test
* Use `t_test_virg_set$estimate` to view the means and `t_test_virg_set$conf.int` to view the 95% confidence interval values<br>
Can we reject the null hypothesis of this test? Is the difference in means larger when comparing *virginica* with *versicolor* or with *setosa*?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 6
#Code
t_test_virg_set <- t.test(virg_pl, set_pl)
t_test_virg_set$p.value

t_test_virg_vers <- t.test(virg_pl, vers_pl)

t_test_virg_vers$estimate
t_test_virg_set$estimate

t_test_virg_vers$conf.int
t_test_virg_set$conf.int

#Answer
#The null hypothesis is that the two means are equal (no difference in length in petal length between the two species).
#Because p-value < 0.05 we reject the null hypothesis.
#The difference in means is much larger when comparing virginica to setosa
```
<br><br>

## Non-paramteric: Wilcoxon test

We have seen how to perform a t-test to compare if the mean of two groups is the same. However, we assumed that our data was normally distributed. Large outliers can heavily influence the sample mean and standard deviation, which would influence the t-test result. But what if during recording of the petal lengths the wrong values had been added? 
<br><br>
Let's change three values to simulate such outliers:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
vers_pl[1] <- 15
vers_pl[2] <- 17
vers_pl[3] <- 14
```
<br>
Now run the t-test again:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
t.test(virg_pl,vers_pl)
```
The t-test is no longer significant.
<br>

**Q7**: 

* Run the wilcoxon test on the same data (`wilcox.test`, type `?wilcox.test` to see examples of how to use it)
* Is the p-value lower or higher? (type `?wilcox.test` for examples how to use it)
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 7
#Code
wilcox.test(virg_pl, vers_pl)

#Answer
#The p-value is highly significant and much lower than the p-value from the t-test. It appears this test deals better with outliers.
```
<br>

The Wilcoxon test merges the data together, ranks each point from lowest to highest values, separates the ranks back to the two groups, and using the sum or average rank calculates the test-statistics. 

**Q8**: 

* Make a plot of the values (including the changes you just added) using the code below
* Read the stripchart help page
* When would you typically use a stripchart?

```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
stripchart(list(virg_pl, vers_pl),
           vertical=TRUE,
           ylab="Observations",
           pch=21,
           bg=1)
```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 8
#Code
stripchart(list(virg_pl, vers_pl),
           vertical=TRUE,
           ylab="Observations",
           pch=21,
           bg=1)
#Answer
#The plots are used to show the differences between two groups when the sample size is small

```
<br>

As you can see in your plot, there is a big gap between the outlier points that we just introduced and the normal points. 
<br>

**Q9**: 

* Make two plots of the real and ranked values using the code below
* Why is the wilcoxon test better for data with outliers?
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
# par() is a function that allows multiple plots to be combined in one figure
# 1 = the number of rows, 2 = the number of columns
par(mfrow=c(1,2))

#select the first 50 numbers using seq(along=virg_pl)
#the along argument gives the number (50) of observations you want to select
#select the last 50 numbers using the negative seq: -seq(along=virg_pl)
xrank <- rank(c(virg_pl,vers_pl))[seq(along=virg_pl)]
yrank <- rank(c(virg_pl,vers_pl))[-seq(along=virg_pl)]

# plot the previous plot for comparison
stripchart(list(virg_pl, vers_pl),
           vertical=TRUE,
           ylab="Observations",
           pch=21,
           bg=1)
stripchart(list(xrank,yrank),
           vertical=TRUE,
           ylab="Ranks",
           pch=21,bg=1,
           cex=1.25)

```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 9
#Code
par(mfrow=c(1,2))

xrank <- rank(c(virg_pl,vers_pl))[seq(along=virg_pl)]
yrank <- rank(c(virg_pl,vers_pl))[-seq(along=vers_pl)]
# plot the previous plot for comparison
stripchart(list(virg_pl, vers_pl),
           vertical=TRUE,
           ylab="Observations",
           pch=21,
           bg=1)
abline(h=0)
stripchart(list(xrank,yrank),
           vertical=TRUE,
           ylab="Ranks",
           pch=21,bg=1,
           cex=1.25)

#Answer
#Because it makes the gap between outliers and the other data smaller, ensuring that those points are not the most important in calculating the differences between the two groups.
```
<br><br>

# Differences between more than 2 groups
Sometimes, we want to know the difference between not just 2 groups, but between more than 2 groups. For example, in the **iris** data, there are 3 species of flower that we can investigate. Of course we can run a t-test or Wilcoxon test for each of the combinations, but we can also use an analysis of variance (ANOVA) or the non-parametric Krukal-Wallis test.
<br><br>

## Parametric: ANOVA
If you want to compare the difference of mean values among multiple groups, you can use the ANOVA test. We want to examine whether the mean value of `Sepal.Length` differs among the different species of **iris**. We use the function `aov()` and we assign the result to `model1`. <br>
The tilde (`~`) must be read as the phrase 'is modeled as a function of'. In this example, we would say: The length of iris sepals is modeled as a function of the iris species.

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
model1 <- aov(Sepal.Length~Species, data = iris)
```
<br>

In general, the basic notation for a linear model or function notation in R is:

```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
response_variable ~ explanatory_variables
```
<br>

**Q10**:

* Use the function `summary()` to have a look at `model1`
* The p-value is in the column `Pr(>F)`
* Is the p-value significant for this test?
* What is the significance code for this test, and what does that mean?
* The degrees of freedom are in the column `Df`
* How many degrees of freedom are there for the `Species`?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 10
#Code
summary(model1)

#Answer
#The p-value is highly signifciant: <2e-16 
#The significance code is ***, which indicates a level of below 0.001
#There are 2 degrees of freedom, that makes sense because there are 3 species (df = variables-1)
```
<br><br>

## Non-parametric: Kruskal-Wallis test
The same test without the assumption of normality (i.e. non-parametric test) is called the Kruskal-Wallis test.
<br>

**Q11**:

* Use the function `kruskal.test()` to calculate the differences between the **length** of the sepals across the three species
* Save the output in `model2`
* Use the function `kruskal.test()` to calculate the differences between the **width** of the sepals across the three species
* Save the output in `model3`
* Is there a difference in significance between `model1` and `model2`?
* What is the p-value for `model3`?
* What can you conclude about the relationship between sepal length and iris species? Is the species important for the sepal length? How about for the sepal width?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 11
#Code
model2 <- kruskal.test(Sepal.Length~Species, data = iris)
model3 <- kruskal.test(Sepal.Width~Species, data = iris)

summary(model1)[[1]][["Pr(>F)"]]
model2$p.value

model3$p.value


#Answer
#P-values of model1 and model2 are both highly significant, model1 a bit more so
#model3 p-value is 1.569282e-14, also significant but less so than the sepal length model
#Both length and width of sepal is dependent on the species of iris

```
<br><br>

# Differences between paired samples
In other instances, you might be interested to understand the differences between samples that are linked (or 'paired') to one another. For example, if you have an experiment with two measurements before and after a treatment from the same sample. Because you are looking at data from the same individual, the measurements before and after will be 'paired'.
<br><br>

## Parametric: paired t-test
We will use gene expression data for this part of the tutorial. The data describes an experiment where cells from a B cell line (from one individual, GM12878) are stimulated with the cytokine IL21. The same cells are sequenced before treatment, after 3 hours with cytokine stimulation and after 3 hours without cytokine stimulation.
<br>

**Q12**:

* Copy the following commands to import and examine the data
* Fill in the missing parts:
* The dataset contains gene expression for the sample GM12878 on **?** genes
  * *Hint*: the 'ENSG....' numbers are gene identifiers
* There are **?** columns (experiments)
* The time points used in this experiment are t=0 and t=180: **TRUE/FALSE**
* The experiment has been done *in triplo*: **TRUE/FALSE**

```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
#load the necessary packages
library(stringr)

# read in the table containing the read counts of all samples that we will use for the analysis
exp <- read.csv("DIRECTORY WHERE YOU SAVED THE DATA/BigData_backup_GM12878-IL21.csv", header=TRUE)
# to continue with the table, we need to create rownames of the column named "probes" in which the gene_IDs are located
rownames(exp) <- exp$X
exp <- exp[,-1]
head(exp)
dim(exp)
```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 12
#Code
library(stringr)
exp <- read.csv("/Users/anniqueclaringbould/Downloads/Data/BigData_backup_GM12878-IL21.csv", header=TRUE)
rownames(exp) <- exp$X
exp <- exp[,-1]

#Answer
# The dataset contains gene expression for the sample GM12878 on 39666 genes
# There are 6 columns (experiments)
# The time points used in this experiment are t=0 and t=180: TRUE
# The experiment has been done *in triplo*: FALSE --> it has been done in duplo (sample A and B)
```
<br>

**Q13** Using this dataset, we can look for specific and a-specific changes in gene expression as a consequence of stimulation by comparing the following data points: <br>

1. Unstimulated & 180 min without cytokine
2. 180 min without cytokine & 180 min with cytokine
3. Unstimulated & 180 min with cytokine

<br>Which comparison will be most reliable and informative if we want to know the effects of IL21 stimulation?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 13
#Answer
# The most informative will be comparison 2) because it will show the specific effects of the cytokine stimulation without adding the effect of preparing the sample and letting it sit for 3 hours. Comparison 1) will indicate how much gene expression changes due to the experimental protocol without the actual stimulation. Comparison 3) does not take this into account.
```
<br>

**Q14** Run a paired t-test to see if there is a significant difference between the stimulated and unstimulated gene expression after 3 hours in sample A:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
t.test(exp$GM12878_A_t180unstim, exp$GM12878_A_t180, paired = TRUE)
```
Now test the difference between unstimulated at the start and unstimulated after 3 hours. Are the paired t-tests significant? What is the biggest difference?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 14
#Code
t.test(exp$GM12878_A_t180unstim, exp$GM12878_A_t180, paired = TRUE)
t.test(exp$GM12878_A_t0unstim, exp$GM12878_A_t180unstim, paired = TRUE)

#Answer
#Both paired t-tests are significant
#The difference between unstimulated at t=0 and at t=180 is much larger (~60.9) than between stimulated and unstimulated at t=180 (~-15)
```
<br><br>

## Non-parametric: Wilcoxon signed rank test
The non-parametric version of the paired t-test is called the Wilcoxon signed rank test. 

**Q15**:

* Run a Wilcoxon signed rank test using `wilcox.test()` on `GM12878_A_t0unstim` and `GM12878_A_t180unstim`
* What happens if you do **not** tell R that the sample information is paired?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 15
#Code
wilcox.test(exp$GM12878_B_t0unstim, exp$GM12878_B_t180unstim, paired = TRUE)
wilcox.test(exp$GM12878_B_t0unstim, exp$GM12878_B_t180unstim, paired = FALSE)

#Answer
#The difference is significant if you tell R it is paired, not if you do not give that information
```
<br><br>

# Association between variables
Until now we have discussed how to test if there is a statistically significant difference between two groups, multiple groups or paired samples within a group. Statistical analyses also often revolve around finding association between variables. For example, in the **iris** dataset, we could wonder whether the length of the petals is associated to the width of the petals, and whether knowledge about one of those two measurements is predictive for the other. <br>

In this section, we will discuss *association between discrete variables*, *correlation*, and *linear models* to identify and examine the association between variables.
<br><br>

## Association between discrete variables
If you want to know the relationship between two categorical variables (also known as discrete data), you can use the Chi-square test or the Fisher's exact test. Categorical variables, as the name implies, are variables that indicate membership of a category (gender: M/F, Iris species: *virginica* / *versicolor* / *setosa*). Categorical variables are called factors in R (see yesterday's info on data types).
<br>

Imagine that you have some data on the food preferences (vegetarian vs. non-vegetarian) and favourite sport to play (soccer vs. volleyball) of a group of 50 people. You can think of this data as a 2x2 table (also called a [contingency table](https://en.wikipedia.org/wiki/Contingency_table#Example)) with counts for each of the preferences for both variables:

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=FALSE}
set.seed(111222)
sport_vector <- c('soccer', 'volleyball')
food_vector <- c('veg', 'non-veg')

sport <- sample(sport_vector, 50, replace = T, prob = c(0.8, 0.2))
food <- sample(food_vector, 50, replace = T, prob = c(0.3, 0.7))

table(sport, food)
#summary(table(sport, food))
```
<br>

You might want to figure out whether soccer players are more likely to be vegetarian, and as you can appreciate in the table, that means you compare the proportion of vegetarian soccer players to the proportion of vegetarion volleyball players.
<br><br>

### Chi-square test
The Chi-square test allows you to test whether there is a relationship between two variables, but it does not tell you about the direction and size of the relationship.<br>
*Null Hypothesis*: There is no relationship between the two variables
<br>
When you reject the null hypothesis with a Chi-Square, you are saying that there is a relationship between the two variables.

The Chi-square test is used to find if two nominal or ordinal (both categorical) variables are independent. R has a built-in dataset called **survey** with student smoking and exercise habits. Load in the dataset:
<br>

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
library(MASS) 
head(survey)
```
<br>

**Q16**

* Construct a contingency table for smoking and exercise by filling in the **???**: `table(survey$???, survey$???)`
* Save the result as `tblSmoke`
* How many individuals are in the contingency table?
* Run a Chi-square test on this sample using `chisq.test`
* What is the null hypothesis and do you reject it?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 16
#Code
tblSmoke = table(survey$Smoke, survey$Exer)
sum(tblSmoke)
chisq.test(tblSmoke)

#Answer
#236 students
#Null hypothesis: smoking habit is independent of the exercise level of the students
#As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis
```
<br>

**Q17**

* Use ```?survey``` to see the description of the columns
* Run a chi-square test to test if which hand is on top after clapping and exercise levels are independent
* What is the null hypothesis and do you reject it (at 0.05 significance level)?
* Do you think this is correct?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 17
#Code
tblClap <- table(survey$Clap, survey$Exer)
chisq.test(tblClap)

#Answer
#As the p-value 0.009816 is lower than the .05 significance level, we reject the null hypothesis that which hand is on top after clapping is independent of the exercise level of the students. 
#This seems like it might be due to some spurious relation; it seems like there is no logical explanation for this dependence
```
<br><br>

### Fisher's exact test
When you ran the Chi-square test, you may have seen this warning:<br>
`Warning message:
In chisq.test(tblClap) : Chi-squared approximation may be incorrect`
<br><br>
The Chi-square test gives an estimation of the p-value, so the approximation could be incorrect. The Fisher's exact test gives (as the name implies) the exact value of the p-value. There is some discussion about whether it is better to use a Chi-square or Fisher's exact test. Traditionally, it was not possible to calculate the exact value (by hand), so the latter has only been used regularly since statistics are done on computers. When the numbers in the contingency table are small (i.e. one of the cells has a value of < 5), it is often advised to use Fisher's exact test.
<br>

**Q18**: Redo the Chi-square exercises but instead use the Fisher Exact test. What are the differences? *Hint*: see `?fisher.test` for information how to use the function.

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 18
#Code
fisher.test(tblSmoke)
fisher.test(tblClap)

#Answer
#Slight less significant p-value in chi-square test of moking behaviour, slightly more significant in test of clapping hands
#DP-values remain in the same order of magnitude
```
<br><br>

## Correlation 
A correlation is a statistic that indicates how two variables are related to one another. In the **iris** dataset, we hyopthesize that the wider a plants petals are, the longer they get. We would like to test whether we have evidence for this hypothesis. Often plotting the two variables will already tell you if they correlate. 
<br><br>

### Parametric: Pearson correlation
Make a scatterplot between Sepal length and Sepal width:
<br>
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
plot(iris$Sepal.Length, iris$Sepal.Width)
```
<br>
This plot indicates there might be a slight negative correlation between the Sepal length and Sepal width.
<br><br>

**Q19**: 

* What is the correlation coefficient belonging to the plot above? Use the function `cor()`
* What is the possible range of correlation coefficients (i.e. what is the maximum and minimum possible correlation between any pair of variables)?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 19
#Code
cor(iris$Sepal.Length, iris$Sepal.Width)

#Answer
#The correlation is -0.1176
#The correlation coefficient must always lie between -1 and 1
```
<br>

**Q20**: 

* What correlation coefficient do you get if you correlate Sepal length of *versicolor* with the Sepal width of *versicolor*?
* Make a scatterplot using `plot()` 
* Why is the correlation coefficient so different (size and direction)?
* Use the function `cor.test()` to test whether the correlation is significantly different from 0
* Is the correlation significant?
* What is the associated p-value?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 20
#Code
cor(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length)
plot(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length)
cor.test(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length)

#Answer
#Correlation is 0.52
#Before we tried to look at the sepal width and length without taking the species into account
#When you look within the species, it becomes clear that the correlation is actually quite strong
#The correlation is significant with a p-value of 8.772e-05
```
<br>

**Q21**:

* Instead of calculating the correlation between two columns at a time, it is also possible to give a whole dataframe input for the correlation function
* Select the columns with numeric values from the **iris** dataframe by using `iris[1:4]`
* Get the correlation coefficients for all numeric columns in the iris dataframe and call it `cormat`
* The output of this command is called a 'correlation matrix'. Why are some values in this matrix 1?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 21
#Code
cormat <- cor(iris[1:4])
cormat

#Answer
#The cells with a correlation of 1 are showing the correlation of the variable with itself, which is 1 by definition
```
<br>

**Q22**:

* Make a scatterplot for the 4 characteristics using `plot()` again (the result should look like below)
* Add colours to each of the species with with `col=iris$Species`
* Compare the correlation matrix and the plots
* Do you think the negative correlation coefficients that you see in the matrix are a good reflection of the relationships between the variables (e.g. between Petal length and Sepal width)?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=FALSE}
plot(iris[1:4], col=iris$Species)
```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 22
#Code
plot(iris[1:4], col=iris$Species)

#Answer
#The negative values show the correlation without taking species into account. It is clear that within each species of iris, there is a (slight) positive correlation. It's always good to do some plotting before you run statistical tests.
```
<br><br>

### Non-parametric: Spearman rank correlation
The non-parametric version of the Pearson correlation is called the Spearman rank correlation. Like the Wilcoxon test, this function first ranks all observations and then calculates the correlations across the ranks, rather than across the real values.
<br>
The default of `cor()` is to use the parametric Pearson correlation, but you can change the method of correlation by adding `method = 'spearman'` to your command.
<br><br>

**Q23**: Answer **Q20** again, but this time using the Spearman rank correlation

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 23
#Code
cor(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length, method = 'spearman')
plot(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length)
cor.test(iris[iris$Species=='versicolor',]$Sepal.Width, iris[iris$Species=='versicolor',]$Sepal.Length, method = 'spearman')

#Answer
#Correlation is 0.517, so very similar to before
#Before we tried to look at the sepal width and length without taking the species into account
#When you look within the species, it becomes clear that the correlation is actually quite strong
#The correlation is significant with a p-value of 0.0001184
```
<br><br>

## Linear models
Now that we have seen that the petal width a petal length is strongly correlated we can use regression analysis to predict values that we have no measured. Regression finds a relationship between the predictor variable (measured in experiments) and response variable (derived from predictor variable). If there is a linear relationship between these two variables they can be represented by a line. The regression formula is

`y = ax + b`

where y is the response variable, x is the predictor variable, and a and b are the coefficients. We want to predict petal length using the petal width. 

A linear model is fitted using the function `lm()` as follows:
<br>
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
modelname <- lm(formula, data=mydata)
```
<br>

In the case of the **iris** dataset, this would for example look like this (where `m1` stands for model 1):
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
m1 <- lm(Petal.Length ~ Petal.Width, data=iris)
```
<br>

**Q24**: Using the results from `m1`, write down the mathematical equation to calculate petal length given the petal width (fill in the values of `a` and `b`)

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 24
#Code
m1 <- lm(Petal.Length ~ Petal.Width, data=iris)

#Answer
#Petal Length = 2.230 * Petal Width + 1.084
```
<br>

**Q25**: Examine the model output by using `summary(m1)`. This command will show you the formula, the quartiles of the residual values, the coefficient estimates, standard errors and p-values, the residual standard error, the explained variance and the F-statistic. How much of the variation in petal length is explained by the variation in petal width?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 25
#Code
summary(m1)

#Answer
#The R-squared (explained variance) is 0.9271, so ~ 92% of the variation in petal length is explained by the variation in petal width.
```
<br>

**Q26**: 

* Given this strong association, you can predict the petal length from the petal width using `predict.lm()`
* Imagine you have measured another iris flower, and its petal width is 4 cm
* Make a new data frame with one measurement: `measured <- data.frame(Petal.Width=4)`
* Predict what the petal length would be for this measurement using `predict.lm(???, ???)`
* The first argument is the model name
* The second argument is the name of the data frame with the measurement
* What is the predicted petal length?
* Save the predicted length as `predicted`

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 26
#Code
measured <- data.frame(Petal.Width=4)
predicted <- predict.lm(m1,measured)
print(predicted)

#Answer
#The petal length is predicted to be 10 centimeters
```
<br>

We want to plot the observed points and regression line to have a look at how well it fits:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
# Plot the chart
m1 <- lm(Petal.Length ~ Petal.Width, data=iris)
plot(iris$Petal.Width,iris$Petal.Length, abline(m1))
```
<br>

**Q27**: 

* Run the same plot again, but this time add specific ranges for the x- and y-axes by adding:
  * `xlim=c(0,4)`
  * `ylim=c(0,11)`
* Add the extra (predicted) point to the plot using `points(4,predicted, col="red")`
* *Hint*: this last line has to be outside of the brackets of `plot`

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 27
#Code
plot(iris$Petal.Width,iris$Petal.Length,
  abline(m1), 
  xlim=c(0,4), 
  ylim=c(0,11))
points(4,predicted, col="red")
```
<br>

**Q28**: Try the same thing but without `xlim` and `ylim` in the plot function. What happens?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 28
#Code
plot(iris$Petal.Width,iris$Petal.Length,
  abline(m1))
points(4,predicted, col="red")

#Answer
#The point falls outside of the plotting area and therefore does not show up
```
<br><br>

# Dimensionality reduction
Most of the statistics we have dealt with so far are suitable for both small and large datasets. However, when dealing with larger datasets, there are specific methods used to cluster and summarise the data. Typically we call data *big data* when a dataset has many samples and many observations per sample, like for example when we have information on the expression of thousands of genes per individual. Data with information on many variables is sometimes called high dimensional data. In the last part of today's tutorial we will return to the gene expression dataset to calculate principal components and run some clustering.
<br><br>

## Normalization and data visualization
In order to perform certain downstream analyses, such as principal component analysis (PCA), or clustering, we need to properly normalise the expression counts first.
<br>

Let's start by saving some information regarding the gene expression dataset as `colData`:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
# Use the information in the column names to generate a data.frame comprising the sample information.
library(stringr)

names = colnames(exp)
colData <- data.frame(row.names = names,
                      type = str_split_fixed(names, "_", 3)[,1],
                      sample = str_split_fixed(names, "_", 3)[,2],
                      stimulation = str_split_fixed(names, "_", 3)[,3])

# Add library size, which is the total ammount of gene reads per sample
colData$libSize <- colSums(exp)

# Make sure that the colnames of your count table are matching with the row names of your colData
all(rownames(colData) %in% colnames(exp))
```
<br>

**Q29**: What information can you find in the newly made column `libSize`? What's the minimum and maximum value of `libSize`?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 29
#Code
max(colData$libSize)
min(colData$libSize)

#Answer
#This column shows the total number of reads per sample
#The minimum is 5840615, the maximum is 8255943
```
<br>

Install and load the following packages: **DESeq2**, **ggplot2**, **ggsci**, **ggrepel**, **RColorBrewer**, **pheatmap**
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("GenomeInfoDbData")

library(GenomeInfoDbData)

if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

BiocManager::install("DESeq2")

#or try
source("https://bioconductor.org/biocLite.R")
biocLite("DESeq2")
```

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=FALSE}
# Load packages
library(DESeq2)
library(ggplot2)
library(ggsci)
library(ggrepel)
library(RColorBrewer)
library(pheatmap)
```
<br>

There are multiple ways in which we can approximate our counts to a normal-like distribution. The **DESeq2** package provides two built-in functions to normalise your data:

* Regularized logarithm, or rlog 
* Variance stabilizing transformations, or VST

These two transformations can accurately corect for the different conditions for which the cells have been treated.
Select `blind = FALSE` to not include the experimental design information. 

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
dds_stim <- DESeqDataSetFromMatrix(countData = exp,
                              colData = colData,
                              design = ~ stimulation+sample)

vst_stim <- assay(vst(dds_stim, blind=FALSE))
```
<br><br>

## Principal component analysis (PCA)
PCA will help assess which samples are more closely related to each other, and how much variation is found between samples. 
<br>

Follow the next steps to include the observations at t=0 unstimulated, t=180 unstimulated and t=180 stimulated in 1 PCA plot.

**Q30**:

* Calculate PCs using `prcomp(t(vst_stim))`
* Save these PCs as `pcData`
* The importance of each component is displayed in `summary(pcData)`
* What proportion of Variance does the first PC explain?
* Save the importance of components as `pcVar`
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 30
#Code
# To calculate the components by sample we need to transpose our matrix of normalized gene expression 
pcData <- prcomp(t(vst_stim))
pcVar <- summary(pcData)
# By getting the summary() of a prcomp object (in this case pcData) we can also obtain the total ammount of variance explained by each of the components.

#Answer
#The variance of PC1 is 0.5726 (or 57%)
```
<br>

**Q31**:

* Extract the importance of PC1 using `pcVar$importance[2,1]`
* Save this number as `varPC1`
* Do the same for PC2 
* Make a data frame for plotting: `pcPlotData <- data.frame(pcData$x[,1:2], colData[rownames(pcData$x),])`
* Examine `pcPlotData` using `head()`
* What information do you have in this data frame?

```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 31
#Code
# We can then extract the variance explained by components 1 and 2. 
varPC1 <- pcVar$importance[2,1]
varPC2 <- pcVar$importance[2,2]
pcPlotData <- data.frame(pcData$x[,1:2], colData[rownames(pcData$x),])
#head(pcPlotData)

#Answer
#Per sample, there is information on their loadings of PC1 and PC2, the sample, stimulation and library size
```
<br>

Make the plot using this code:
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
pcaPlot_stim <- ggplot(pcPlotData, aes(x=PC1 , y=PC2 , color=stimulation))+
                  geom_jitter(alpha=0.6)+
                  xlab(paste0("PC1 explained variance = ", varPC1*100, "%"))+
                  ylab(paste0("PC2 explained variance = ", varPC2*100, "%"))+
                  scale_color_aaas()+
                  theme_bw()+
                  theme(legend.position = "bottom")+
                  guides(col = guide_legend(ncol = 8))

# display the pca plot in "Plots"
pcaPlot_stim
```
<br><br>

Use the following code to save your plot:
```{r, message=FALSE, warning=FALSE, eval=FALSE, echo=TRUE}
# to save your file to pdf format in your working directory do
pdf("DIRECTORY WHERE YOU SAVED THE DATA/PCA_GM12878.pdf", height = 7.5, width=7.5)
pcaPlot_stim
dev.off()
```
<br>

Clearly each of the duplicated of the three conditions cluster together, and there is quite some difference between t0unstim and t180unstim. However, the difference between the t180 and t180unstim samples is still much greater. This is the stimulation effect, which is what we are interested in.
<br>

**Q32 (bonus)**: Make the PC plot, this time color the dots by the `sample` instead of the `stimulation`. Can you also observe a pattern here?
```{r, message=FALSE, warning=FALSE, eval=TRUE, echo=TRUE}
#Question 32
#Code
pcaPlot_sample <- ggplot(pcPlotData, aes(x=PC1 , y=PC2 , color=sample))+
                  geom_jitter(alpha=0.6)+
                  xlab(paste0("PC1 explained variance = ", varPC1*100, "%"))+
                  ylab(paste0("PC2 explained variance = ", varPC2*100, "%"))+
                  scale_color_aaas()+
                  theme_bw()+
                  theme(legend.position = "bottom")+
                  guides(col = guide_legend(ncol = 8))

# display the pca plot in "Plots"
pcaPlot_sample

#Answer
#No, the samples are nicely clustering together and there does not seem to be an effect of sample
```
<br><br>

# Unsupervised clustering
Alternatively, it's possible to assess the similarity between samples using an unsupervised clustering approach using the whole trancriptome to generate a distance matrix. 

**Q33**: Follow the next steps to make clusters and show a heatmap:
```{r, message=FALSE, eval=FALSE, echo=TRUE}
# Again we need to transpose our matrix to calculate the distance between each of the samples.
sampleDists <- dist(t(vst_stim))
sampleDistMatrix <- as.matrix(sampleDists)

# By using brewer.pal() we can generate a palette of colors, for more colors check (http://colorbrewer2.org/)
colors <- colorRampPalette(brewer.pal(9, "GnBu"))(255)

pheatmap(sampleDistMatrix, main = "Stimulation",
         show_colnames = FALSE,
         annotation = colData[,c("stimulation","sample")],
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors)
```
What is the biggest difference in these experiments: the sample (A/B), the time (t=0 / t=180) or the stimulation (stimulated / unstimulated)? *Hint*: Look at the dendogram on the left of the graph.  
<br>

```{r, message=FALSE, eval=TRUE, echo=TRUE, fig.height= 5, fig.width= 5}
#Question 33
#Code
# Again we need to transpose our matrix to calculate the distance between each of the samples.
sampleDists <- dist(t(vst_stim))
sampleDistMatrix <- as.matrix(sampleDists)

# By using brewer.pal() we can generate a palette of colors, for more colors check (http://colorbrewer2.org/)
colors <- colorRampPalette(brewer.pal(9, "GnBu"))(255)

pheatmap(sampleDistMatrix, main = "Stimulation",
         show_colnames = FALSE,
         annotation = colData[,c("stimulation","sample")],
         clustering_distance_rows=sampleDists,
         clustering_distance_cols=sampleDists,
         col=colors)

#Answer
#The heatmap shows a similar results to the PCA: the biggest difference is the time point, closely followed by the stimulation. The sample has (almost) no influence.
```
<br><br><br><br><br><br>
